{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "np.random.seed(1)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--pooling POOLING]\n                             [--n_epochs N_EPOCHS] [--sample_size SAMPLE_SIZE]\n                             [--dim DIM] [--k K] [--batch_size BATCH_SIZE]\n                             [--l2_weight L2_WEIGHT] [--lr LR]\nipykernel_launcher.py: error: unrecognized arguments: -f /home/nolaurence/.local/share/jupyter/runtime/kernel-2bffde0a-53fc-4353-94fa-c80ea84858e3.json\n"
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='kaggle', help='set aside')\n",
    "parser.add_argument('--pooling', type=str, default='adamic', help='which pooling method to use')\n",
    "parser.add_argument('--n_epochs', type=int, default=10, help=' the number of epochs')\n",
    "parser.add_argument('--sample_size', type=int, default=3, help='the number of child node of every node')\n",
    "parser.add_argument('--dim', type=int, default=32, help='number of embedding vector, choose in [8, 16, 32]')\n",
    "parser.add_argument('--k', type=int, default=3, help='the depth of tree')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
    "parser.add_argument('--l2_weight', type=float, default=1e-6, help='weight of l2 regularization in 1e-6~1')\n",
    "parser.add_argument('--lr', type=float, default=1e-2, help='learning rate')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSCN(args, n_users, n_items, adj_item, adj_adam, user2item_dict)\n",
    "def load_data():\n",
    "    user2item = np.load('newdata/user2item.npy', allow_pickle=True).item()\n",
    "    users = set(user2item.keys())\n",
    "    items = list(np.load('newdata/items.npy', allow_pickle=True))\n",
    "    n_item = len(items)\n",
    "    items = list(range(len(items)))\n",
    "    n_user = len(users)\n",
    "    \n",
    "    adj_item = np.load('newdata/adj_item.npy', allow_pickle=True)\n",
    "    adj_adam = np.load('newdata/daj_adam.npy', allow_pickle=True)\n",
    "    user2item = np.load('newdata/user2item.npy', allow_pickle=True).item()\n",
    "    train_data = np.load('newdata/traindata.npy', allow_pickle=True)\n",
    "    test_data = np.load('newdata/testdata.npy', allow_pickle=True)\n",
    "    return n_user, n_item, items, adj_item, adj_adam, user2item, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化与卷积步骤的具体函数\n",
    "LAYER_IDS = {}\n",
    "def get_layer_id(layer_name=''):\n",
    "    if layer_name not in LAYER_IDS:\n",
    "        LAYER_IDS[layer_name] = 0\n",
    "        return 0\n",
    "    else:\n",
    "        LAYER_IDS[layer_name] += 1\n",
    "        return LAYER_IDS[layer_name]\n",
    "\n",
    "\n",
    "class Adamicpooling(object):\n",
    "    def __init__(self, batch_size, dim, dropout-0., act=tf.nn.relu, name=None):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + str(get_layer_id(layer))\n",
    "        sself.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.name = name\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weights1 = tf.get_variable(shape=[self.dim, self.dim],\n",
    "                                           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           name='weights')\n",
    "            self.bias1 = tf.get_variable(shape=[self.dim],\n",
    "                                        initializer=tf.zeros_initializer(),\n",
    "                                        name='bias')\n",
    "    def __call__(self, self_vectors, neighbor_vectors, neighbor_adams):\n",
    "        outputs = self._call(self_vectors, neighbor_vectors, neighbor_adams)\n",
    "        return outputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call(self, self_vectors, neighbor_vectors, neighbor_adams):\n",
    "        # self_vectors:根节点的特征向量每个点自己本身的特征向量:[batch_size, -1, dim]\n",
    "        # neighbor_vectors:邻点的特征向量:[batch_size, -1, n_sample, dim]\n",
    "        # neighbor_adams:权重值:[batch_size, -1, n_sample]\n",
    "\n",
    "        # normalize adamic values [batch_size, -1, n_sample]\n",
    "        adamvalues_normalized = tf.nn.softmax(neighbor_adams, dim=-1)\n",
    "        # [batch_size, -1, n_sample, 1]\n",
    "        adamvalues_normalized = tf.expanda_dims(adamvalues_normalized, axis=-1)\n",
    "        # [batch_size, -1, dim]\n",
    "        neighbors_afterPooling = tf.reduce_mean(adamvalues_normalized * neighbor_vectors, axis=2)\n",
    "        # [batch_Size, -1, dim*2]\n",
    "        output = tf.concat([self_vectors, neighbors_afterPooling], axis=-1)\n",
    "        # [-1, dim*2]\n",
    "        output = tf.reshape(output, [-1, self.dim * 2])\n",
    "        output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
    "        # [-1. dim] full connect method\n",
    "        output = tf.matmul(output, self.weights) + self.bias\n",
    "        # [batch_size, -1, dim]\n",
    "        output = tf.reshape(output, [self.batch_size, -1, self.dim])\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class AveragePooling(object):\n",
    "    def __init__(self, batch_size, dim, dropout=0., act=tf.nn.relu, name=None):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_id(layer))\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.name = name\n",
    "        wit tf.variable_scope(self.name):\n",
    "            self.weights = tf.get_variable(shape=[self.dim, self.dim], \n",
    "                                           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           name='weights')\n",
    "            self.bias = tf.get_variable(shape=[self.dim], \n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                        name='bias')\n",
    "    def __call__(self, self_vectors, neighbor_vectors):\n",
    "        outputs = self._call(self_vectors, neighbor_vectors)\n",
    "        return outputs\n",
    "\n",
    "    @abtractmethod\n",
    "    def _call(self, self_vectors, neighbor_vectors):\n",
    "        # self_vectors:每个点自己本身的特征向量:[batch_size, -1, dim]\n",
    "        # neighbor_vectors:每个点子节点的特征向量:[batch_size, -1, n_sample, dim]\n",
    "\n",
    "        # [batch_size, -1, dim]\n",
    "        neighbors_afterPooling = tf.reduce_mean(neighbor_vectors, axis=-1)\n",
    "        # [batch_Size, -1, dim*2]\n",
    "        output = tf.concat([self_vectors, neighbors_afterPooling], axis=-1)\n",
    "        # [-1, dim*2]\n",
    "        output = tf.reshape(output, [-1, self.dim * 2])\n",
    "        output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
    "        # [-1. dim] full connect method\n",
    "        output = tf.matmul(output, self.weights) + self.bias\n",
    "        # [batch_size, -1, dim]\n",
    "        output = tf.reshape(output, [self.batch_size, -1, self.dim])\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class Maxpooling(object):\n",
    "    def __init__(self, batch_size, dim, dropout=0., act=tf.nn.relu, name=None):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_id(layer))\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.name = name\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.weights = tf.get_variable(shape=[self.dim, self.dim],\n",
    "                                           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           name='weights')\n",
    "            self.bias = tf.get_variable(shape=[self.dim],\n",
    "                                        initializer=tf.zeros_initializer(),\n",
    "                                        name='bias')\n",
    "            \n",
    "    def __call__(self, self_vectors, neighbor_vectors):\n",
    "        outputs = self._call(self_vectors, neighbor_vectors)\n",
    "        return outputs\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call(self, self_vectors, neighbor_vectors):\n",
    "        # self_vectors: [batch_size, -1, dim]\n",
    "        # neighbor_vectors: [batch_size, -1, n_sample, dim]\n",
    "\n",
    "        # [batch_size, -1, dim]\n",
    "        neighbors_afterPooling = tf.reduce_max(neighbor_vectors, axis=-1)\n",
    "        # [batch_size, -1, dim*2]\n",
    "        output = tf.concat([self_vectors, neighbors_afterPooling], axis=-1)\n",
    "        # [-1, dim*2]\n",
    "        output = tf.reshape(output, [-1, self.dim * 2])\n",
    "        output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
    "        # [-1. dim] full connect method\n",
    "        output = tf.matmul(output, self.weights) + self.bias\n",
    "        # [batch_size, -1, dim]\n",
    "        output = tf.reshape(output, [self.batch_size, -1, self.dim])\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class fc_layer(object):\n",
    "    def __init__(self, batch_size, dim, dropout=0., act=tf.nn.softmax, name=None):\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_id(layer))\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.name = name\n",
    "        wit tf.variable_scope(self.name):\n",
    "            self.wfc = tf.get_variable(shape=[self.dim, self.dim], \n",
    "                                           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           name='weights')\n",
    "            self.bfc = tf.get_variable(shape=[self.dim], \n",
    "                                        initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                        name='bias')\n",
    "            self.wd = tf.get_variable(shape=[self.dim], initializer=tf.contrib.layers.xavier_initializer(), name='weights2')\n",
    "            # self.bd = tf.get_variable(shape=[self.dim])\n",
    "    def __call__(self, user_embeddings, item_embeddings):\n",
    "        outputs = self._call(user_embeddings, item_embeddings)\n",
    "        return outputs\n",
    "\n",
    "    def _call(self, user_embeddings, item_embeddings):\n",
    "        # [batch_size, dim*2]\n",
    "        output1 = tf.concat([user_embeddings, item_embeddings], axis=-1)\n",
    "        # [-1, dim*2]\n",
    "        output1 = tf.reshape(output1, [-1, self.dim * 2])\n",
    "        output1 = tf.nn.dropout(output1, keep_prob=1-self.dropout)\n",
    "        # [-1, dim] full connect layer\n",
    "        output1 = tf.matmul(output1, self.wfc) + self.bfc\n",
    "        # [batch_size, dim]\n",
    "        output1 = tf.reshape(output1, [self.batch_size, self.dim])\n",
    "        output1 = tf.nn.relu(output1)\n",
    "\n",
    "        # output layer\n",
    "        output2 = tf.reshape(output1, [-1, self.dim])\n",
    "        output = tf.matmul(output2, self.wd)\n",
    "        output = reshape(output, [self.batch_size])\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSCN(object):\n",
    "    def __init__(self, args, n_users, n_items, adj_item, adj_adam, user2item_dict):\n",
    "        self._parse_args(args, adj_item, adj_adam， user2item_dict)\n",
    "        self._build_inputs()\n",
    "        self._build_model(args, n_users, n_items, n_adamvalues)\n",
    "        self._build_train()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_initializer():\n",
    "        return tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    def _parse_args(self, args, adj_item, adj_adam, user2item_dict):\n",
    "        self.adj_item = adj_item\n",
    "        self.adj_adam = adj_adam\n",
    "        self.user2item_dict = user2item_dict\n",
    "\n",
    "        self.k = args.k\n",
    "        self.batch_size = args.batch_size\n",
    "        self.n_sample = args.sample_size  # the number of sample a vertex's childnode\n",
    "        self.dim = args.dim\n",
    "        self.l2_weight = args.l2_weight\n",
    "        self.lr = args.lr\n",
    "        if args.pooling == 'average':\n",
    "            self.pooling_class = AveragePooling\n",
    "        elif args.pooling == 'max':\n",
    "            self.pooling_class = MaxPooling\n",
    "        elif args.pooling == 'adamic':\n",
    "            self.pooling_class = AdamicPooling\n",
    "        else:\n",
    "            raise Exception('Unknown pooling method: ' + args.pooling)\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        # ndices = tf.placeholder(dtype=tf.int32, shape=[None], name='user_indices')\n",
    "        self.user_indices = tf.placeholder(dtype=tf.int32, shape=[None], name='user_indices')\n",
    "        self.item_indices = tf.placeholder(dtype=tf.int32, shape=[None], name='item_indices')\n",
    "        self.labels = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n",
    "\n",
    "\n",
    "    # 此处为计算图\n",
    "    # 2019/12/21 完成\n",
    "    def _build_model(self, args, n_users, n_items, n_adamvalues):\n",
    "        self.item_embedding_matrix = tf.get_variable(\n",
    "            shape=[n_items, self.dim], initializer=TSCN.get_initializer(), name='item_embedding_matrix')\n",
    "        # self.user_embedding_matrix = self.user_emb_init(shape=[n_users, self.dim], initializer=TSCN.get_initializer(), name='user_embedding_matrix')\n",
    "        \n",
    "        # 解释：为何这里的大小是batch_size x dim:\n",
    "        # 在训练的过程中，我们输入的user item interaction graph分为三行\n",
    "        # 第一行为user 第二行为item 第三行为label（positive‘s label = 1,negative's label=0)\n",
    "        # 当数据处理完成后即开始训练\n",
    "        # [batch_size, dim]\n",
    "        self.item_embeddings = tf.nn.embedding_lookup(self.item_embedding_matrix, self.item_indices)\n",
    "        self.user_embeddings = tf.nn.embedding_lookup(self.user_embedding_matrix, self.user_indices)\n",
    "        # 用户特征向量初始化\n",
    "        self.user_embeddings = self.user_emb_initializer(self.user_indices)\n",
    "\n",
    "        entities, adamvalues = self.get_childnodes(self.item_indices)\n",
    "        # pooling step\n",
    "        self.item_embeddings, self.poolings = self.pool_and_convolution(args, entities, adamvalues)\n",
    "        # finished the itemgraph convolution step\n",
    "        # [batch_size, dim]\n",
    "        # fc layer\n",
    "        self.fclayer = fc_layer(self.batch_size, self.dim)\n",
    "        self.output = self.fclayer(self.user_embeddings, self.itemembeddings)\n",
    "\n",
    "    # 添加了一个n_users, 用户id的\n",
    "\n",
    "    # 用户向量的初始化函数\n",
    "    def user_emb_initializer(self, userindex, itemindex):\n",
    "        print('initializing user embeddings ...')\n",
    "        i = 0\n",
    "        uservec = []\n",
    "        for u in userindex:\n",
    "            itemids = self.user2item_dict[u]\n",
    "            if itemindex[0] in itemids:\n",
    "                itemids.remove(itemindex[0])\n",
    "                length = len(itemids)\n",
    "                itemids = tf.Tensor(itemids)\n",
    "                itemvectors = tf.gather(self.adj_item, itemids)\n",
    "                user_emb = tf.divide(tf.reduce_sum(itemvectors, axis=-1), length)\n",
    "            else:\n",
    "                itemvectors = tf.gather(self.adj_item, itemids)\n",
    "                user_emb = tf.divide(tf.reduce_sum(itemvectors, axis=-1), length)\n",
    "            uservec.append(user_emb)\n",
    "        output = tf.reshape(uservec, [self.batch_size, self.dim])\n",
    "        return output\n",
    "        \n",
    "\n",
    "    def get_childnodes(self, seeds):\n",
    "        print('geting childnodes of vertexes ...')\n",
    "        seeds = tf.expand_dims(seeds, axis=1)\n",
    "        entities = [seeds]\n",
    "        adamvalues = []\n",
    "        for i in range(self.k):\n",
    "            neighbor_entities = tf.reshape(tf.gather(self.adj_item, entities[i]), [self.batch_size, -1])\n",
    "            neighbor_adamvalues = tf.reshape(tf.gather(self.adj_adam, entities[i]), [self.batch_size, -1])\n",
    "            entities.append(neighbor_entities)\n",
    "            adamvalues.append(neighbor_adamvalues)\n",
    "        return entities, adamvalues\n",
    "    \n",
    "    # 这个函数需要传入adamvalues的量 12/09 22:19 fixed\n",
    "    def pool_and_convolution(self, args, entities, adamvalues):\n",
    "        poolings = [] # store all pooling method\n",
    "        item_vector = [tf.nn.embedding_lookup(self.item_embedding_matrix, i) for i in entities]\n",
    "        # adam_vectors = [tf.nn.embedding_lookup(self.adam_embedding_matrix, i) for i in adamvalues]\n",
    "        \n",
    "        if args.pooling == 'adamic':\n",
    "            for i in range(self.k):\n",
    "                pooling = self.pooling_class(self.batch_size, self.dim)\n",
    "                poolings.append(pooling)\n",
    "\n",
    "                item_vector_next_iter = []\n",
    "                for hop in range(self.k - i):\n",
    "                    # dimension explanation: batchsize x nodes x numberOfSample x dimension of embedding vector\n",
    "                    # the number of nodes is uncertain, so it's -1\n",
    "                    # why: in iteration 1 n_nodes=1, in iteration 2 n_nodes=3 ...\n",
    "                    shape = [self.batch_size, -1, self.n_sample, self.dim]\n",
    "                    vector = pooling(self_vectors=item_vector[hop], \n",
    "                                     neighbor_vectors=tf.reshape(item_vector[hop + 1], shape)， \n",
    "                                     neighbor_adams=tf.reshape(adamvalues[hop], [self.batch_size, -1, self.n_sample]))\n",
    "                    item_vectors_next_iter.appen(vector)\n",
    "            item_vectors = item_vectors_next_iter\n",
    "        else:\n",
    "            for i in range(self.k):\n",
    "                pooling = self.pooling_class(self.batch_size, self.dim)\n",
    "                poolings.append(pooling)\n",
    "\n",
    "                item_vector_next_iter = []\n",
    "                for hop in range(self.k - i):\n",
    "                    shape = [self.batch_size, -1, self.n_sample, self.dim]\n",
    "                    vector = pooling(self_vectors=item_vector[hop], \n",
    "                                     neighbor_vectors=tf.reshape(item_vector[hop + 1], shape))\n",
    "                    item_vectors_next_iter.appen(vector)\n",
    "            item_vectors = item_vectors_next_iter\n",
    "        \n",
    "        res = tf.reshape(item_vectors[0], [self.batch_size, self.dim])\n",
    "        return res, poolings\n",
    "    def _build_train(self):\n",
    "        # 计算损失函数\n",
    "        self.base_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=self.output))\n",
    "        self.l2_loss = tf.nn.l2_loss(self.user_embedding_matrix) + tf.nn.l2_loss(self.item_embedding_matrix)\n",
    "        for p in self.poolings:\n",
    "            self.l2_loss += tf.nn.l2_loss(p.weights)\n",
    "        self.l2_loss += tf.nn.l2_loss(self.fclayer.wfc) + tf.nn.l2_loss(self.fclayer.wd)\n",
    "        self.loss = self.base_loss + self.l2_weight * self.l2_loss\n",
    "\n",
    "        # 优化器\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        return sess.run([self.optimizer, self.loss], feed_dict)\n",
    "    \n",
    "    def eval(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.output], feed_dict)\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        scores[scores >= 0.5] = 1\n",
    "        scores[scores < 0.5] = 0\n",
    "        f1 = f1_score(y_true=labels, y_pred=scores)\n",
    "        return auc, f1\n",
    "    \n",
    "    def get_scores(self, sess, feed_dict):\n",
    "        return sess.run([self.item_indices, self.output], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算过程代码\n",
    "# __init__(self, args, n_users, n_items, adj_item, adj_adam, user2item_dict):\n",
    "# 2020/01/10 userid indexed itemid indexed\n",
    "\n",
    "# 以下是TSCN的输入：\n",
    "# def _build_inputs(self):\n",
    "#         # ndices = tf.placeholder(dtype=tf.int32, shape=[None], name='user_indices')\n",
    "#         self.user_indices = tf.placeholder(dtype=tf.int32, shape=[None], name='user_indices')\n",
    "#         self.item_indices = tf.placeholder(dtype=tf.int32, shape=[None], name='item_indices')\n",
    "#         self.labels = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n",
    "def get_feed_dict(model, data, start, end):\n",
    "    feed_dict = {model.user_indices: data[start:end, 0],\n",
    "                 model.item_indices: data[start:end, 1],\n",
    "                 model.labels: data[start:end, 2]}\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "# is_train的类型为bool\n",
    "def get_user_record(data, is_train):\n",
    "    user_history_dict = dict()\n",
    "    for interaction in data:\n",
    "        user = interaction[0]\n",
    "        item = interaction[1]\n",
    "        label = interaction[2]\n",
    "        if is_train or label == 1:\n",
    "            if user not in user_history_dict:\n",
    "                user_history_dict[user] = set()\n",
    "            user_history_dict[user].add(item)\n",
    "    return user_history_dict\n",
    "\n",
    "def topn_settings(train_data, test_data, n_item):\n",
    "    train_record = get_user_record(train_data, True)\n",
    "    test_record = get_user_record(test_data, False)\n",
    "    user_list = list(set(train_record.keys()) & set(test_record.keys()))\n",
    "    item_set = set(list(range(n_item)))\n",
    "    return user_list, train_record, test_record, item_set\n",
    "\n",
    "\n",
    "def topn_eval(sess, model, user_list, train_record, test_record, item_set, n, batch_size):\n",
    "    HR_precision_list = list()\n",
    "    NDCG_precision_list = list()\n",
    "\n",
    "    Z = 0\n",
    "    for i in range(n):\n",
    "        Z += 1 / (math.log2(i + 2))\n",
    "    Z = 1 / Z\n",
    "\n",
    "    for user in user_list:\n",
    "        test_item_list = list(item_set - train_record[user])\n",
    "        item_score_map = dict()\n",
    "        start = 0\n",
    "        while start + batch_size <= len(test_item_list):\n",
    "            items, scores = model.get_score(sess, {model.user_indices: [user] * batch_size, model.item_indices: test_item_list[start:start + batch_size]})\n",
    "            for item, score in zip(items, scores):\n",
    "                item_score_map[item] = score\n",
    "            start += batch_size\n",
    "        \n",
    "        # padding the last incomplete minibatch if exists\n",
    "        if start < len(test_item_list):\n",
    "            items, scores = model.get_scores(sess, {model.user_indices: [user] * batch_size, \n",
    "                                                    model.item_indices: test_item_list[start:] + [test_item_list[-1]] * (batch_size - len(test_item_list) + start)})\n",
    "            for item, score in zip(items, scores):\n",
    "                item_score_map[item] = score\n",
    "        \n",
    "        item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n",
    "        item_sorted = [i[0] for i in item_score_pair_sorted]\n",
    "        \n",
    "        hit_num = len(set(item_sorted[:n]) & test_record[user])\n",
    "        # HR result\n",
    "        precision1 = hit_num / n\n",
    "        HR_precision_list.append(precision1)\n",
    "        \n",
    "        # NDCG result\n",
    "        sum = 0\n",
    "        recommended_list = list(item_sorted[:n])\n",
    "        hit_set = set(item_sorted[:n]) & test_record[user]\n",
    "        for i in len(recommended_list):\n",
    "            if recommended_list[i] in hit_set:\n",
    "                sum += 1 / (math.log2(i + 2))\n",
    "        precision2 = Z * sum\n",
    "        NDCG_precision_list.append(precision2)\n",
    "    HR_precision = np.mean(HR_precision_list)\n",
    "    NDCG_precision = np.mean(NDCG_precision_list)\n",
    "    return HR_precision, NDCG_precision\n",
    "\n",
    "\n",
    "def train(args, data, show_loss):\n",
    "    # load_data返回值：return n_user, n_item, items, adj_item, adj_adam, user2item, train_data, test_data\n",
    "    n_user = data[0]\n",
    "    n_item = data[1]\n",
    "    items = data[2]\n",
    "    adj_item = data[3]\n",
    "    adj_adam = data[4]\n",
    "    user2item = data[5]\n",
    "    train_data, test_data = data[6], data[7]\n",
    "    model = TSCN(args, n_users=n_user, n_items=n_item, adj_item=adj_item, adj_adam=adj_adam, user2item_dict=user2item)\n",
    "    # 2019/12/21 14:50 :现在按照要求重新处理原始数据\n",
    "    # topK evaluation settings 论文中貌似没有 忽略\n",
    "    \n",
    "    user_list, train_record, test_record, item_set = topn_settings(train_data, test_data, n_item)\n",
    "\n",
    "    # 训练过程\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(args.n_epochs):\n",
    "            # training\n",
    "            np.random.shuffle(train_data)\n",
    "            start = 0\n",
    "            while start + args.batch_size <= train_data.shape[0]:\n",
    "                _, loss = model.train(sess, get_feed_dict(model, train_data, start, start + args.batch_size))\n",
    "                start += args.batch_size\n",
    "                if show_loss:\n",
    "                    print(start, loss)\n",
    "            # evalution method should be added. (2020/01/10 20:30)\n",
    "            # HR evaluation step\n",
    "            # def topn_eval(sess, model, user_list, train_record, test_record, item_set, n, batch_size):\n",
    "            HR_precision, NDCG_precision = topn_eval(sess, model, user_list, train_record, test_record, item_set, 10, args.batch_size)\n",
    "            print('epoch {} '.format(step))\n",
    "            print('HR precision: {:.4f} '.format(HR_precision), end='')\n",
    "            print('NDCG precision: {:.4f}'.format(NDCG_precision))\n",
    "\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "train(args, data=data, show_loss=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TSCN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda9ae3fa8cc0f848339da46840240e490d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}